{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.data import Dataset\n",
    "from spektral.data.graph import Graph\n",
    "from spektral.data import DisjointLoader\n",
    "from spektral.layers import ECCConv, GlobalSumPool\n",
    "from spektral.data.loaders import BatchLoader\n",
    "from spektral.models.general_gnn import GeneralGNN\n",
    "from spektral.utils import tic, toc\n",
    "from spektral.layers.convolutional import gcn_conv\n",
    "from spektral.layers.convolutional import arma_conv\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from numpy.random import rand\n",
    "from numpy.random import randint\n",
    "from random import randrange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# suppress tensorflow log messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 100\n",
    "Y = 100\n",
    "N_GRAPHS = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10000)\n"
     ]
    }
   ],
   "source": [
    "G = nx.grid_2d_graph(X, Y)\n",
    "inp = rand(G.number_of_nodes(), 1)\n",
    "out = inp / 10\n",
    "\n",
    "a = nx.to_numpy_array(G, dtype=np.float32)\n",
    "\n",
    "print(a.shape)\n",
    "\n",
    "nx.set_node_attributes(G, out, 'value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Features(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset for the features test.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nodes, feats, graphs, adjacency, **kwargs):\n",
    "        self.nodes = nodes\n",
    "        self.feats = feats\n",
    "        self.graphs = graphs\n",
    "        self.a = adjacency\n",
    "        self.mask_tr = self.mask_va = self.mask_te = None\n",
    "\n",
    "        self.download()\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def download(self):\n",
    "        data = ...  # Download from somewhere\n",
    "\n",
    "        # Create the directory\n",
    "        if not os.path.exists(self.path):\n",
    "            os.mkdir(self.path)\n",
    "\n",
    "        # Write the data to file\n",
    "        for i in range(self.graphs):\n",
    "            x = rand(G.number_of_nodes(), self.feats).astype(np.float32)\n",
    "            a = self.a\n",
    "            y = x / 10\n",
    "            e = np.full((G.number_of_nodes(), G.number_of_nodes(),\n",
    "                        1), 10.0, dtype=np.float32)\n",
    "\n",
    "            filename = os.path.join(self.path, f'graph_{i}')\n",
    "            np.savez(filename, x=x, a=a, e=e, y=y)\n",
    "\n",
    "        mask = np.zeros(self.nodes, dtype=np.bool8)\n",
    "\n",
    "        self.mask_tr = mask\n",
    "        self.mask_tr[0:int(self.nodes // (5/4))] = True\n",
    "        self.mask_va = mask\n",
    "        self.mask_va[int((self.nodes // (5/4)) + 1)\n",
    "                         :int(self.nodes // (10/9))] = True\n",
    "        self.mask_te = mask\n",
    "        self.mask_te[int((self.nodes // (10/9)) + 1):self.nodes] = True\n",
    "\n",
    "    def read(self):\n",
    "        # We must return a list of Graph objects\n",
    "        output = []\n",
    "\n",
    "        for i in range(self.graphs):\n",
    "            data = np.load(os.path.join(self.path, f'graph_{i}.npz'))\n",
    "            output.append(\n",
    "                Graph(x=data['x'], a=data['a'], y=data['y'], e=data['e'])\n",
    "            )\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = Features((X * Y), 1, N_GRAPHS, a)\n",
    "graph = ds[0]\n",
    "x, a, y = graph.x, graph.a, graph.y\n",
    "\n",
    "mask_tr, mask_va, mask_te = ds.mask_tr, ds.mask_va, ds.mask_te\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(n_nodes=10000, n_node_features=1, n_edge_features=1, n_labels=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThisGCN(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    This model, with its default hyperparameters, implements the architecture\n",
    "    from the paper:\n",
    "    > [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907)<br>\n",
    "    > Thomas N. Kipf and Max Welling\n",
    "    **Mode**: single, disjoint, mixed, batch.\n",
    "    **Input**\n",
    "    - Node features of shape `([batch], n_nodes, n_node_features)`\n",
    "    - Weighted adjacency matrix of shape `([batch], n_nodes, n_nodes)`\n",
    "    **Output**\n",
    "    - Softmax predictions with shape `([batch], n_nodes, n_labels)`.\n",
    "    **Arguments**\n",
    "    - `n_labels`: number of channels in output;\n",
    "    - `channels`: number of channels in first GCNConv layer;\n",
    "    - `activation`: activation of the first GCNConv layer;\n",
    "    - `output_activation`: activation of the second GCNConv layer;\n",
    "    - `use_bias`: whether to add a learnable bias to the two GCNConv layers;\n",
    "    - `dropout_rate`: `rate` used in `Dropout` layers;\n",
    "    - `l2_reg`: l2 regularization strength;\n",
    "    - `**kwargs`: passed to `Model.__init__`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_labels,\n",
    "        channels=16,\n",
    "        activation=\"relu\",\n",
    "        output_activation=\"relu\",\n",
    "        use_bias=False,\n",
    "        dropout_rate=0.5,\n",
    "        l2_reg=2.5e-4,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.n_labels = n_labels\n",
    "        self.channels = channels\n",
    "        self.activation = activation\n",
    "        self.output_activation = output_activation\n",
    "        self.use_bias = use_bias\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.l2_reg = l2_reg\n",
    "        reg = tf.keras.regularizers.l2(l2_reg)\n",
    "        self._d0 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self._arma0 = arma_conv.ARMAConv(\n",
    "            channels, activation=activation, kernel_regularizer=reg, use_bias=use_bias\n",
    "        )\n",
    "        self._d1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self._arma1 = arma_conv.ARMAConv(\n",
    "            n_labels, activation=output_activation, use_bias=use_bias\n",
    "        )\n",
    "        self.norm = tf.keras.layers.Normalization()\n",
    "\n",
    "    def get_config(self):\n",
    "        return dict(\n",
    "            n_labels=self.n_labels,\n",
    "            channels=self.channels,\n",
    "            activation=self.activation,\n",
    "            output_activation=self.output_activation,\n",
    "            use_bias=self.use_bias,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            l2_reg=self.l2_reg,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if len(inputs) == 2:\n",
    "            x, a = inputs\n",
    "        else:\n",
    "            x, a, _ = inputs  # So that the model can be used with DisjointLoader\n",
    "\n",
    "        #x = self._d0(x)\n",
    "        #x = self._arma0([x, a])\n",
    "        #x = self._d1(x)\n",
    "        x = self._arma1([x, a])\n",
    "        #x = self.norm(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/anaconda3/envs/spektral/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = ThisGCN(n_labels=ds.n_labels, output_activation='relu',\n",
    "                use_bias=False, channels=8, activation='relu')\n",
    "model.compile('adam', 'mean_squarred_error')\n",
    "optimizer = Adam(lr=1e-3, beta_1=0.9, beta_2=0.999)\n",
    "loss_fn = MeanSquaredError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "@tf.function\n",
    "def train():\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model([x, a], training=True)\n",
    "        # tf.keras.backend.eval(predictions)\n",
    "        loss = loss_fn(y, predictions)\n",
    "        loss += sum(model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss = 5.400334543992358e-07\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "# train()  # Warm up to ignore tracing times when timing\n",
    "# tic()\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    loss = train()\n",
    "    # print(f'{epoch}/{EPOCHS + 1}')\n",
    "print(f\"Final loss = {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model([ds[0].x, ds[0].a], training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp \t\t target \t prediction\n",
      "[0.09152373] \t [0.00915237] \t [0.00964277]\n",
      "[0.9299479] \t [0.09299479] \t [0.09159435]\n",
      "[0.9526359] \t [0.09526359] \t [0.09397903]\n",
      "[0.27284595] \t [0.0272846] \t [0.0273614]\n",
      "[0.22789101] \t [0.0227891] \t [0.02308998]\n",
      "[0.9432238] \t [0.09432238] \t [0.09316468]\n",
      "[0.782502] \t [0.0782502] \t [0.07733753]\n",
      "[0.07967036] \t [0.00796704] \t [0.00913838]\n",
      "[0.88714087] \t [0.08871409] \t [0.08699435]\n",
      "[0.02020095] \t [0.0020201] \t [0.00329766]\n",
      "[0.7813558] \t [0.07813558] \t [0.07664625]\n",
      "[0.40511513] \t [0.04051151] \t [0.04082731]\n",
      "[0.93637955] \t [0.09363796] \t [0.09264733]\n",
      "[0.88402045] \t [0.08840205] \t [0.0874404]\n",
      "[0.7902052] \t [0.07902052] \t [0.07859133]\n",
      "[0.9621381] \t [0.09621381] \t [0.09507436]\n",
      "[0.5246358] \t [0.05246358] \t [0.05228867]\n",
      "[0.6705537] \t [0.06705537] \t [0.06624425]\n",
      "[0.3199243] \t [0.03199243] \t [0.03210551]\n",
      "[0.7687994] \t [0.07687994] \t [0.07596046]\n"
     ]
    }
   ],
   "source": [
    "print(f'inp \\t\\t target \\t prediction')\n",
    "for i in range(0, min(ds[0].n_nodes, 20)):\n",
    "    print(f'{ds[0].x[i]} \\t {ds[0].y[i]} \\t {pred[i]}')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "205bf0bf3312de3a55af1929d317d37b296e42bb604722daab0731b294172f11"
  },
  "kernelspec": {
   "display_name": "Python [conda env:spektral]",
   "language": "python",
   "name": "conda-env-spektral-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
